name: Daily WARN scrape

on:
  schedule:
    - cron: '0 15 * * *' # 10:00 AM EST (15:00 UTC)
  workflow_dispatch:

permissions:
  contents: write

jobs:
  scrape:
    name: Scrape
    runs-on: ubuntu-latest
    continue-on-error: true
    strategy:
      fail-fast: false
      matrix:
        # TEST TIP: Replace the list below with [ca, ia, ny] to test quickly
        state: [ia, ny]
        # state: [al, ak, az, ar, ca, co, ct, de, fl, ga, hi, id, il, in, ia, ks, ky, la, me, md, ma, mi, mn, ms, mo, mt, ne, nv, nh, nj, nm, ny, nc, nd, oh, ok, or, pa, ri, sc, sd, tn, tx, ut, vt, va, wa, wv, wi, wy]
    
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install scraper
        run: pip install warn-scraper

      - name: Scrape
        # Using the direct python execution fix to avoid Exit Code 127
        run: |
          mkdir -p ./data/
          python -c "from warn.cli import main; import sys; sys.argv=['warn-scraper', '${{ matrix.state }}', '--data-dir', './data/']; main()"

      - name: upload-artifact
        uses: actions/upload-artifact@v4
        with:
          name: ${{ matrix.state }}
          path: ./data/${{ matrix.state }}.csv

  commit:
    name: Commit
    runs-on: ubuntu-latest
    needs: scrape
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Download artifact
        uses: actions/download-artifact@v4
        with:
          pattern: '*'
          path: artifacts/

      - name: Move
        run: |
          mkdir -p data
          # This moves all CSVs from their individual artifact folders into /data
          mv artifacts/**/*.csv data/

      - name: Create Rolling 30-Day Window
        shell: python
        run: |
          import pandas as pd
          import glob
          from datetime import datetime, timedelta

          # Define the window (e.g., 30 days)
          threshold_date = datetime.now() - timedelta(days=30)
          all_data = []

          # Loop through all state CSVs we just moved
          for file in glob.glob("./data/*.csv"):
              if "recent_notices" in file: continue # Skip the output file itself
              
              df = pd.read_csv(file)
              # Standardize date column (find column with 'date' in name)
              date_col = next((c for c in df.columns if 'date' in c.lower()), None)
              
              if date_col:
                  df[date_col] = pd.to_datetime(df[date_col], errors='coerce')
                  # Filter for records newer than the threshold
                  recent = df[df[date_col] >= threshold_date]
                  all_data.append(recent)

          # Combine all states into one "Recent" file for journalists
          if all_data:
              final_df = pd.concat(all_data)
              final_df.sort_values(by=date_col, ascending=False, inplace=True)
              final_df.to_csv("./data/recent_notices.csv", index=False)

      - name: Save datestamp
        # Per the tutorial, this logs the time and ensures the commit isn't empty
        run: date > ./data/latest-scrape.txt

      - name: Commit and push
        run: |
          git config user.name "GitHub Actions"
          git config user.email "actions@users.noreply.github.com"
          git add ./data/
          git commit -m "Latest data" && git push || true
