name: Matrix WARN Scraper with Slack Alerts

on:
  schedule:
    - cron: '0 15 * * *' # 10:00 AM EST
  workflow_dispatch:

permissions:
  contents: write

jobs:
  scrape:
    name: Scrape
    runs-on: ubuntu-latest
    continue-on-error: true
    strategy:
      fail-fast: false
      max-parallel: 20 # Limit parallelism to prevent GitHub from throttling
      matrix:
        # TEST TIP: Use [ca, ny, ia] for your first run
        # state: [ny, ia]
        state: [al, ak, az, ca, co, ct, de, fl, ga, hi, id, il, in, ia, ks, me, md, mo, mt, ne, nj, nm, ny, ok, or, pa, ri, sc, sd, tn, tx, ut, vt, va, wa, wi]
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'
          cache: 'pip' # Speeds up the run significantly

      - name: Install scraper
        run: pip install warn-scraper

      - name: Scrape
        timeout-minutes: 10 # Prevents a single state from hanging the whole run
        run: |
          mkdir -p ./data/
          python -c "from warn.cli import main; import sys; sys.argv=['warn-scraper', '${{ matrix.state }}', '--data-dir', './data/']; main()"

      - name: upload-artifact
        uses: actions/upload-artifact@v4
        with:
          name: ${{ matrix.state }}
          path: ./data/${{ matrix.state }}.csv

  commit:
    name: Commit and Alert
    runs-on: ubuntu-latest
    needs: scrape
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0 # Fetch all history so rebase works correctly

      - name: Install Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'
          cache: 'pip'

      - name: Install Dependencies
        run: pip install pandas requests

      - name: Download artifact
        uses: actions/download-artifact@v4
        with:
          pattern: '*'
          path: artifacts/

      - name: Move and Prepare
        run: |
          mkdir -p data
          # Store the 'old' version of the files before overwriting them
          mkdir -p old_data
          cp data/*.csv old_data/ 2>/dev/null || true
          mv artifacts/**/*.csv data/

      - name: Create Rolling 30-Day Window
        shell: python
        run: |
          import pandas as pd
          import glob
          import os
          from datetime import datetime, timedelta

          threshold = datetime.now() - timedelta(days=30)
          all_dfs = []
          
          for f in sorted(glob.glob("./data/*.csv")):
              if "recent_notices.csv" in f: continue
              try:
                  df = pd.read_csv(f)
                  
                  # Find relevant columns
                  d_col = next((c for c in df.columns if 'date' in c.lower()), None)
                  c_col = next((c for c in df.columns if any(x in c.lower() for x in ['company', 'employer'])), None)
                  
                  if d_col:
                      df[d_col] = pd.to_datetime(df[d_col], errors='coerce')
                      # Filter 1: Date is within 30 days
                      df = df[df[d_col] >= threshold]
                      
                  if c_col:
                      # Filter 2: Company is not Unknown or empty
                      df = df[df[c_col].notna()]
                      df = df[~df[c_col].astype(str).str.lower().isin(['unknown', 'nan', 'n/a', ''])]
                  
                  all_dfs.append(df)
              except Exception:
                  continue
                  
          if all_dfs:
              final_recent = pd.concat(all_dfs)
              # Final sort so the newest stuff is always at the top
              final_recent.sort_values(by=d_col, ascending=False, inplace=True)
              final_recent.to_csv("./data/recent_notices.csv", index=False)

      - name: Send Slack Alerts
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
        shell: python
        run: |
          import pandas as pd
          import requests
          import os
          import glob
          from datetime import datetime

          URL_MAP = {
              "NY": "https://dol.ny.gov/warn-notices",
              "CA": "https://edd.ca.gov/en/Jobs_and_Training/Layoff_Services_WARN",
              "IA": "https://www.iowaworkforcedevelopment.gov/worker-adjustment-and-retraining-notification-warn-notices",
              "TX": "https://www.twc.texas.gov/biz/warn/warn-notices.html"
          }

          webhook_url = os.environ.get('SLACK_WEBHOOK_URL')
          if not webhook_url:
              exit(0)

          for new_file in glob.glob("./data/*.csv"):
              if "recent_notices.csv" in new_file: continue
              state_code = os.path.basename(new_file).replace('.csv', '').upper()
              old_file = new_file.replace('./data/', './old_data/')
              state_url = URL_MAP.get(state_code, f"https://www.google.com/search?q={state_code}+WARN+notices")
              
              try:
                  df_new = pd.read_csv(new_file)
                  
                  # 1. Identify New Records
                  if os.path.exists(old_file):
                      df_old = pd.read_csv(old_file)
                      merged = df_new.merge(df_old, how='outer', indicator=True)
                      new_records = merged[merged['_merge'] == 'left_only'].drop('_merge', axis=1)
                  else:
                      new_records = df_new.head(5)

                  for _, row in new_records.iterrows():
                      # 2. Advanced Column Mapping
                      # Company Name
                      co = next((str(row[c]) for c in df_new.columns if any(x in c.lower() for x in ['company', 'employer', 'organization'])), "Unknown")
                      
                      # Jobs Affected (Added 'layoff', 'worker', 'total' to search)
                      jobs = next((str(row[c]) for c in df_new.columns if any(x in c.lower() for x in ['job', 'worker', 'affected', 'layoff', 'count', 'number', 'total'])), "N/A")
                      
                      # Notice Date
                      date_val = next((row[c] for c in df_new.columns if 'notice' in c.lower() and 'date' in c.lower()), None)
                      if not date_val:
                          date_val = next((row[c] for c in df_new.columns if 'date' in c.lower()), "N/A")

                      # 3. APPLY FILTERS (The "Guardrails")
                      
                      # Skip if Company is unknown or missing
                      if not co or co.lower() in ['unknown', 'nan', 'n/a', '']:
                          continue

                      # Skip if date is not in 2026 (Fixes the 2003/2025 issue)
                      try:
                          parsed_date = pd.to_datetime(date_val, errors='coerce')
                          if parsed_date.year != 2026:
                              continue
                      except:
                          continue # Skip if date is unparseable

                      # 4. SEND ALERT
                      payload = {
                          "blocks": [
                              {"type": "section", "text": {"type": "mrkdwn", "text": f"*New WARN Notice: {state_code}*\n*Company:* {co}\n*Jobs:* {jobs}\n*Notice Date:* {date_val}"}},
                              {"type": "actions", "elements": [{"type": "button", "text": {"type": "plain_text", "text": "ðŸ”— View State Page"}, "url": state_url}]}
                          ]
                      }
                      requests.post(webhook_url, json=payload)
              except Exception as e:
                  print(f"Error alerting for {state_code}: {e}")

      - name: Update Index Page
        shell: python
        run: |
          import glob
          import os
          from datetime import datetime

          # Professional URL Map
          URL_MAP = {
              "NY": "https://dol.ny.gov/warn-dashboard",
              "CA": "https://edd.ca.gov/en/Jobs_and_Training/Layoff_Services_WARN",
              "IA": "https://workforce.iowa.gov/employers/resources/warn/notices",
              "TX": "https://www.twc.texas.gov/data-reports/warn-notice",
              "FL": "https://www.floridajobs.org/office-directory/division-of-workforce-services/workforce-programs/reemployment-and-emergency-assistance-coordination-team-react/warn-notices",
              "PA": "https://www.pa.gov/agencies/dli/programs-services/workforce-development-home/warn-requirements/warn-notices",
              "NJ": "https://www.nj.gov/labor/business-services/layoffs-and-closing/file-warn-notice/",
              "OH": "https://jfs.ohio.gov/job-services-and-unemployment/job-services/job-programs-and-services/submit-a-warn-notice/current-public-notices-of-layoffs-and-closures-sa/current-public-notices-of-layoffs-and-closures"
          }

          # Keep the professional intro and global timestamp
          content = "# WARN Act Data Portal\n\n"
          content += "Automated tracking of layoff notices across the U.S. Updated daily at 10:00 AM EST.\n\n"
          content += "### Latest Global Update: " + datetime.now().strftime("%Y-%m-%d %H:%M") + " UTC\n\n"
          
          content += "| State | Data Download | Official Source |\n"
          content += "| :--- | :--- | :--- |\n"
          
          # Include the combined 30-day file at the top of the table
          if os.path.exists("./data/recent_notices.csv"):
              content += "| **All States** | [Download Combined 30-Day CSV](./data/recent_notices.csv) | - |\n"

          # Dynamically add only the states that were successfully scraped
          state_files = sorted(glob.glob("./data/*.csv"))
          for f in state_files:
              filename = os.path.basename(f)
              if filename == "recent_notices.csv":
                  continue
              
              state_code = filename.replace('.csv', '').upper()
              source_url = URL_MAP.get(state_code, f"https://www.google.com/search?q={state_code}+official+WARN+notices")
              
              content += f"| {state_code} | [Download CSV](./data/{filename}) | [Official Source]({source_url}) |\n"

          content += "\n\n---\n*This page is automatically updated by GitHub Actions. Data provided by Big Local News.*"

          with open("README.md", "w") as f:
              f.write(content)

      - name: Save datestamp
        run: date > ./data/latest-scrape.txt

      - name: Commit and push
        run: |
          git config user.name "GitHub Actions"
          git config user.email "actions@users.noreply.github.com"
          
          # Add all data files and the updated README
          git add ./data/ README.md
          
          # Only commit if there are actually changes to prevent "empty commit" errors
          if git diff --staged --quiet; then
            echo "No changes detected. Skipping commit."
          else
            git commit -m "Update WARN data and portal index [skip ci]"
            # Use --rebase to handle potential sync issues if multiple jobs finish at once
            git pull --rebase origin main
            git push origin main
          fi
